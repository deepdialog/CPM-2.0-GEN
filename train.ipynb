{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c2635b-d934-461a-b1da-1ece08f19839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import T5EncoderModel\n",
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b0a9ec-206d-4bb8-899c-155da7409235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_enc_dec import EncDecTokenizer\n",
    "tokenizer = EncDecTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "039d82b4-8ffb-4420-b606-f83eea4164bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModel(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.encoder = T5EncoderModel(config)\n",
    "        self.out = torch.nn.Linear(4096, 512)\n",
    "\n",
    "    def forward(self, input_ids, mask):\n",
    "        x = self.encoder(input_ids, mask).last_hidden_state\n",
    "        x = x[:, 0, :]\n",
    "        x = self.out(x)\n",
    "        x = F.normalize(x, p=2, dim=-1)\n",
    "        return x\n",
    "\n",
    "class TrainModel(torch.nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(TrainModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.proj = torch.nn.Linear(512 * 4, 2)\n",
    "\n",
    "    def forward(self, input_ids_0, mask_0, input_ids_1, mask_1):\n",
    "        a = self.encoder(input_ids_0, mask_0)\n",
    "        b = self.encoder(input_ids_1, mask_1)\n",
    "\n",
    "        c = torch.cat([\n",
    "            a,\n",
    "            b,\n",
    "            torch.abs(a - b),\n",
    "            a + b\n",
    "        ], dim=-1)\n",
    "        c = self.proj(c)\n",
    "        \n",
    "        sim = 1 - torch.arccos(\n",
    "            # 切到-1.0和1.0可能导致fp16下的NaN\n",
    "            torch.clip(\n",
    "                torch.sum(a * b, 1),\n",
    "                -0.99,\n",
    "                0.99\n",
    "            )\n",
    "        ) / 3.1415\n",
    "        \n",
    "        return c, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa351b60-281d-48a6-adb6-f9c366361f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from tokenization_enc_dec import EncDecTokenizer\n",
    "tokenizer = EncDecTokenizer('./vocab.txt')\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, batch_size=16, data_root = '../sts_dataset'):\n",
    "        self.batch_size = batch_size\n",
    "        lines = []\n",
    "        for f in tqdm([os.path.join(data_root, f) for f in os.listdir(data_root)]):\n",
    "            lines += open(f).read().split('\\n')\n",
    "        data = []\n",
    "        bad = []\n",
    "        for l in tqdm(lines):\n",
    "            s = l.split('\\t')\n",
    "            if len(s) >= 3 and s[-1] in ('0', '1') and len(s[-3].strip()) >= 1 and len(s[-2].strip()) >= 1:\n",
    "                y = int(s[-1])\n",
    "#                 if y < 1:\n",
    "#                     y = -1\n",
    "                data.append((s[-3], s[-2], y))\n",
    "            else:\n",
    "                bad.append(s)\n",
    "        random.shuffle(data)\n",
    "        self.data = data\n",
    "        print(len(data))\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_size = self.batch_size\n",
    "        batch = []\n",
    "        random.shuffle(self.data)\n",
    "        while True:\n",
    "            for item in self.data:\n",
    "                batch.append(item)\n",
    "                if len(batch) >= batch_size:\n",
    "                    x0 = torch.nn.utils.rnn.pad_sequence([\n",
    "                        torch.LongTensor(\n",
    "                            [1] + tokenizer.encode(\n",
    "                                x[0]\n",
    "                            )\n",
    "                        )\n",
    "                        for x in batch\n",
    "                    ], batch_first=True, padding_value=tokenizer.pad_id)\n",
    "                    x1 = torch.nn.utils.rnn.pad_sequence([\n",
    "                        torch.LongTensor(\n",
    "                            [1] + tokenizer.encode(x[1])\n",
    "                        )\n",
    "                        for x in batch\n",
    "                    ], batch_first=True, padding_value=tokenizer.pad_id)\n",
    "                    y = torch.LongTensor([x[2] for x in batch])\n",
    "                    m0 = (x0 != tokenizer.pad_id).to(torch.int64)\n",
    "                    m1 = (x1 != tokenizer.pad_id).to(torch.int64)\n",
    "                    yield x0, m0, x1, m1, y\n",
    "                    batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59133e22-e0ac-49c0-9aea-7490e615ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config(\n",
    "    vocab_size=26240,\n",
    "#     n_positions=self.n_positions,\n",
    "    d_model=4096,\n",
    "    d_ff=10240,\n",
    "    d_kv=4096 // 64,\n",
    "    num_layers=2,\n",
    "    num_heads=64,\n",
    "    relative_attention_num_buckets=32,\n",
    "    dropout_rate=0.0,\n",
    "    initializer_factor=1.0,\n",
    "    eos_token_id=tokenizer.eod_id,\n",
    "    bos_token_id=tokenizer.pad_id,\n",
    "    pad_token_id=tokenizer.pad_id,\n",
    "    decoder_start_token_id=tokenizer.pad_id,\n",
    "    feed_forward_proj='gated-gelu',\n",
    "    tie_word_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d576bd55-4010-48fe-adcc-405cb3adabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../cpm-2.1-encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "545e90cf-4160-40b6-8880-57b3384c19db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63a9e4b5-84ac-48b4-88e0-563af48f4dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = {\n",
    "    k: v\n",
    "    for k, v in state_dict.items()\n",
    "    if 'encoder.block.' not in k or (\n",
    "        'encoder.block.0.' in k or 'encoder.block.1.' in k\n",
    "    )\n",
    "}\n",
    "new_state_dict['encoder.final_layer_norm.weight'] = model.encoder.state_dict()['encoder.final_layer_norm.weight']\n",
    "model.encoder.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8ae2623-6d5b-4f4e-88b2-bbe1930e2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = TrainModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7486791-147d-4def-8465-7701399236c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.65it/s]\n",
      "100%|██████████| 980227/980227 [00:01<00:00, 521399.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980227\n",
      "CPU times: user 3.28 s, sys: 271 ms, total: 3.55 s\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ds = Dataset(batch_size=64)\n",
    "dl = torch.utils.data.DataLoader(ds, num_workers=4, batch_size=None, pin_memory=True, prefetch_factor=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5de2b94d-8ddf-4bc1-b2d1-06aeccb6d569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 4.45 s, total: 27.3 s\n",
      "Wall time: 435 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fp16 = True\n",
    "cuda = True\n",
    "\n",
    "if cuda:\n",
    "    if fp16:\n",
    "        train_model = train_model.half().cuda()\n",
    "    else:\n",
    "        train_model = train_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f96d5d2b-80d5-4590-a674-011bcba41ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fp16:\n",
    "    optimizer = torch.optim.SGD(\n",
    "        train_model.parameters(),\n",
    "        lr=5e-3,\n",
    "        momentum=0.9\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(\n",
    "        train_model.parameters(),\n",
    "        lr=5e-3,\n",
    "        momentum=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7285982-1e01-4079-8ec5-1de546210458",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fc_0 = torch.nn.NLLLoss()\n",
    "loss_fc_1 = torch.nn.MSELoss()\n",
    "m = torch.nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba3874bc-5e8d-4028-b843-ab7a6848367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "losses = []\n",
    "losses0 = []\n",
    "losses1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9475e6b2-0ac6-48f8-a484-2df33d88d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...Building prefix dict from the default dictionary ...Building prefix dict from the default dictionary ...\n",
      "\n",
      "\n",
      "Loading model from cache /tmp/jieba.cacheLoading model from cache /tmp/jieba.cacheLoading model from cache /tmp/jieba.cache\n",
      "\n",
      "\n",
      "Loading model cost 0.879 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 0.880 seconds.Loading model cost 0.880 seconds.\n",
      "\n",
      "Prefix dict has been built successfully.Prefix dict has been built successfully.\n",
      "\n",
      "Loading model cost 0.886 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "step: 94337 loss: 0.4264 l0: 0.2368 l1: 0.1896: : 368it [01:26,  4.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c15cb82ebd1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fc_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fc_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e3e76282ffef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids_0, mask_0, input_ids_1, mask_1)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-e3e76282ffef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mistgpu/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         )\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mistgpu/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m                 )\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mistgpu/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m             \u001b[0mclamp_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mclamp_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclamp_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(dl)\n",
    "\n",
    "for x0, m0, x1, m1, y in pbar:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if cuda:\n",
    "        x0 = x0.cuda()\n",
    "        m0 = m0.cuda()\n",
    "        x1 = x1.cuda()\n",
    "        m1 = m1.cuda()\n",
    "        y = y.cuda()\n",
    "    with torch.cuda.amp.autocast():\n",
    "        c, sim = train_model(x0, m0, x1, m1)\n",
    "        l0 = loss_fc_0(m(c), y)\n",
    "        l1 = loss_fc_1(sim, y.to(sim.dtype))\n",
    "        loss = l0 + l1\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    l0 = l0.detach().cpu().numpy()\n",
    "    l1 = l1.detach().cpu().numpy()\n",
    "    loss = loss.detach().cpu().numpy()\n",
    "    losses.append(loss)\n",
    "    losses = losses[-100:]\n",
    "    losses0.append(l0)\n",
    "    losses0 = losses0[-100:]\n",
    "    losses1.append(l1)\n",
    "    losses1 = losses1[-100:]\n",
    "    pbar.set_description(f'step: {step} loss: {np.mean(losses):.4f} l0: {np.mean(losses0):.4f} l1: {np.mean(losses1):.4f}')\n",
    "    step += 1\n",
    "    if step > 0 and step % (60 * 60) == 0:\n",
    "        print('save', step, np.mean(losses))\n",
    "        torch.save(model.state_dict(), f'model_{step}.pt')\n",
    "        torch.save(optimizer.state_dict(), f'opt_{step}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365c6c5-973e-4074-a9d3-cc9d04e31109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142d301-5641-4616-9f91-5e1102ae39d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    (e0, m0),\n",
    "    './model.onnx',\n",
    "    opset_version=13,\n",
    "    input_names=[\n",
    "        \"encoder_outputs\", \"mask\",\n",
    "    ],\n",
    "    output_names=[\"normalized_vector\"],\n",
    "    dynamic_axes={\n",
    "        \"encoder_outputs\": {0: \"batch\", 1: \"sequence\"},\n",
    "        \"mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "        \"normalized_vector\": {0: \"batch\", 1: \"sequence\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d46d4-cb07-4adb-8869-b09a304d59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !du -sh './model.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b5760-a46f-4a37-8815-2e9fb503e564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
