{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from transformers import T5EncoderModel, TFT5Model\n",
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_enc_dec import EncDecTokenizer\n",
    "tokenizer = EncDecTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config(\n",
    "    vocab_size=51968,  # en-zh: 51968\n",
    "#     n_positions=self.n_positions,\n",
    "    d_model=4096,\n",
    "    d_ff=10240,\n",
    "    d_kv=4096 // 64,\n",
    "    num_layers=24,\n",
    "    num_heads=64,\n",
    "    relative_attention_num_buckets=32,\n",
    "    dropout_rate=0.0,\n",
    "    initializer_factor=1.0,\n",
    "    eos_token_id=tokenizer.eod_id,\n",
    "    bos_token_id=tokenizer.pad_id,\n",
    "    pad_token_id=tokenizer.pad_id,\n",
    "    decoder_start_token_id=tokenizer.pad_id,\n",
    "    feed_forward_proj='gated-gelu',\n",
    "    tie_word_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5EncoderModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=torch.LongTensor([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(name):\n",
    "    return state_dict[name].numpy()\n",
    "\n",
    "encoder_names0 = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names0 = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "encoder_names = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "def get_block_weight(n, t='encoder', dim=4096):\n",
    "    weights = []\n",
    "    for k, v in state_dict.items():\n",
    "        if t in k and f'blocks.{n}.' in k:\n",
    "            # pytorch和tensorflow版本的weights是矩阵转置的\n",
    "            w = v.numpy()\n",
    "            if 'self_attn.project' in k:\n",
    "                w0, w1, w2 = w[:dim, :], w[dim:dim*2, :], w[dim*2:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "#                 w2 = np.transpose(w2)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "                weights.append((k, w2))\n",
    "            elif 'cross_attn.project_q' in k:\n",
    "#                 w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "            elif 'cross_attn.project_kv' in k:\n",
    "                w0, w1 = w[:dim, :], w[dim:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "            else:\n",
    "#                 if 'dense' in k:\n",
    "#                     w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "    if 'relative_attention_bias' in weights[3][0]:\n",
    "        weights[3], weights[4] = weights[4], weights[3]\n",
    "    weights = [x[1] for x in weights]\n",
    "    if 'encoder' == t:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(encoder_names0 if n == 0 else encoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    else:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(decoder_names0 if n == 0 else decoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../converted.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new_weights = OrderedDict()\n",
    "model_new_weights['shared.weight'] = get_weight('word_embeds.weight')\n",
    "model_new_weights['encoder.embed_tokens.weight'] = get_weight('encoder.word_embeds.weight')\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='encoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['encoder.final_layer_norm.weight'] = get_weight('encoder.final_layernorm.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.state_dict().keys()) - set(model_new_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model_new_weights.keys()) - set(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeds.weight torch.Size([51968, 4096])\n",
      "lm_head.weight torch.Size([51968, 4096])\n",
      "encoder.word_embeds.weight torch.Size([51968, 4096])\n",
      "encoder.final_layernorm.weight torch.Size([4096])\n",
      "encoder.blocks.0.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 64])\n",
      "encoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.0.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.0.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.1.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.1.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.1.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.2.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.2.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.2.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.3.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.3.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.3.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.4.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.4.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.4.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.5.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.5.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.5.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.6.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.6.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.6.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.7.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.7.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.7.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.8.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.8.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.8.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.9.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.9.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.9.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.10.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.10.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.10.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.11.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.11.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.11.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.12.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.12.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.12.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.13.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.13.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.13.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.14.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.14.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.14.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.15.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.15.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.15.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.16.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.16.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.16.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.17.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.17.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.17.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.18.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.18.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.18.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.19.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.19.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.19.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.20.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.20.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.20.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.21.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.21.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.21.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.22.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.22.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.22.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.23.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.23.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.23.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.word_embeds.weight torch.Size([51968, 4096])\n",
      "decoder.final_layernorm.weight torch.Size([4096])\n",
      "decoder.blocks.0.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 64])\n",
      "decoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.0.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.1.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.2.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.3.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.4.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.5.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.6.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.7.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.8.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.9.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.10.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.11.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.12.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.13.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.14.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.15.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.16.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.17.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.18.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.19.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.20.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.21.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.22.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.23.ff.layer_norm.weight torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight torch.Size([51968, 4096])\n",
      "encoder.embed_tokens.weight torch.Size([51968, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 64])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.6.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.7.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.8.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.9.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.10.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.11.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.12.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.13.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.14.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.15.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.16.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.17.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.18.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.19.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.20.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.21.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.22.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.23.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.final_layer_norm.weight torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight (51968, 4096)\n",
      "encoder.embed_tokens.weight (51968, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 64)\n",
      "encoder.block.0.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.0.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.1.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.2.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.3.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.4.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.5.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.6.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.7.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.8.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.9.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.10.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.11.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.12.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.13.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.14.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.15.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.16.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.17.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.18.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.19.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.20.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.21.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.22.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.23.layer.1.layer_norm.weight (4096,)\n",
      "encoder.final_layer_norm.weight (4096,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in model_new_weights.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(model_new_weights) == len(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict({k: torch.from_numpy(v) for k, v in model_new_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.557 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "input_text = '''当地时间9月6日是美国劳工节，但就在这一天，上千万美国劳动者却陷入新的困境。因为美国政府为疫情期间失业者提供的主要救助同日到期，而且白宫表示没有进一步延长救助的计划。\n",
    "在德尔塔变异株已把美国推入新一轮疫情的背景下，失业救济的突然“断供”意味着有上千万美国人将全部或部分失去他们的生活来源。'''\n",
    "input_ids = torch.LongTensor([tokenizer.encode(input_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    input_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 102, 4096])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cpm_2.0_encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.1G\tcpm_2.0_encoder.pt\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh 'cpm_2.0_encoder.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:652: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"tanh\" \"_vml_cpu\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9c3986e21422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m torch.onnx.export(model,               # model being run\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# model input (or a tuple for multiple inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0;34m\"onnx/cpm_2_0_encoder.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# where to save the model (can be a file or file-like object)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0;31m# export_params=True,        # store the trained parameter weights inside the model file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# the ONNX version to export the model to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     return utils.export(model, args, f, export_params, verbose, training,\n\u001b[0m\u001b[1;32m    276\u001b[0m                         \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_raw_ir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0moperator_export_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0moperator_export_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 _model_to_graph(model, args, verbose, input_names,\n\u001b[0m\u001b[1;32m    690\u001b[0m                                 \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                                 \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, example_outputs, _retain_param_name, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mexample_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     graph, params, torch_out, module = _create_jit_graph(model, args,\n\u001b[0m\u001b[1;32m    459\u001b[0m                                                          _retain_param_name)\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args, _retain_param_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_trace_and_get_graph_from_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mtrace_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_states\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mwarn_on_static_input_change\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mONNXTracedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         graph, out = torch._C._create_graph_by_tracing(\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0min_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_return_inputs_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    996\u001b[0m                 )\n\u001b[1;32m    997\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    999\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                 \u001b[0mrecording_scopes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecording_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/activations.py\u001b[0m in \u001b[0;36mgelu_new\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mGaussian\u001b[0m \u001b[0mError\u001b[0m \u001b[0mLinear\u001b[0m \u001b[0mUnits\u001b[0m \u001b[0mpaper\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"tanh\" \"_vml_cpu\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  input_ids,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"onnx/cpm_2_0_encoder.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  # export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=13,          # the ONNX version to export the model to\n",
    "                  verbose=True,\n",
    "                  use_external_data_format=True)  # file size > 2G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh \"onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
