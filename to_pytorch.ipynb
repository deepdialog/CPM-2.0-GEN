{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from transformers import T5EncoderModel, T5ForConditionalGeneration, TFT5Model\n",
    "from transformers import T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_enc_dec import EncDecTokenizer\n",
    "tokenizer = EncDecTokenizer('./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = T5Config(\n",
    "    vocab_size=26240,\n",
    "#     n_positions=self.n_positions,\n",
    "    d_model=4096,\n",
    "    d_ff=10240,\n",
    "    d_kv=4096 // 64,\n",
    "    num_layers=24,\n",
    "    num_heads=64,\n",
    "    relative_attention_num_buckets=32,\n",
    "    dropout_rate=0.0,\n",
    "    initializer_factor=1.0,\n",
    "    eos_token_id=tokenizer.eod_id,\n",
    "    bos_token_id=tokenizer.pad_id,\n",
    "    pad_token_id=tokenizer.pad_id,\n",
    "    decoder_start_token_id=tokenizer.pad_id,\n",
    "    feed_forward_proj='gated-gelu',\n",
    "    tie_word_embeddings=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input_ids=torch.LongTensor([[1]]), decoder_input_ids=torch.LongTensor([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(name):\n",
    "    return state_dict[name].numpy()\n",
    "\n",
    "encoder_names0 = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names0 = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.relative_attention_bias.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "encoder_names = [\n",
    "    'encoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'encoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'encoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_0.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wi_1.weight',\n",
    "    'encoder.block.{}.layer.1.DenseReluDense.wo.weight',\n",
    "    'encoder.block.{}.layer.1.layer_norm.weight',\n",
    "]\n",
    "\n",
    "decoder_names = [\n",
    "    'decoder.block.{}.layer.0.SelfAttention.q.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.k.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.v.weight',\n",
    "    'decoder.block.{}.layer.0.SelfAttention.o.weight',\n",
    "    'decoder.block.{}.layer.0.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.q.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.k.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.v.weight',\n",
    "    'decoder.block.{}.layer.1.EncDecAttention.o.weight',\n",
    "    'decoder.block.{}.layer.1.layer_norm.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_0.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wi_1.weight',\n",
    "    'decoder.block.{}.layer.2.DenseReluDense.wo.weight',\n",
    "    'decoder.block.{}.layer.2.layer_norm.weight',\n",
    "]\n",
    "\n",
    "def get_block_weight(n, t='encoder', dim=4096):\n",
    "    weights = []\n",
    "    for k, v in state_dict.items():\n",
    "        if t in k and f'blocks.{n}.' in k:\n",
    "            # pytorch和tensorflow版本的weights是矩阵转置的\n",
    "            w = v.numpy()\n",
    "            if 'self_attn.project' in k:\n",
    "                w0, w1, w2 = w[:dim, :], w[dim:dim*2, :], w[dim*2:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "#                 w2 = np.transpose(w2)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "                weights.append((k, w2))\n",
    "            elif 'cross_attn.project_q' in k:\n",
    "#                 w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "            elif 'cross_attn.project_kv' in k:\n",
    "                w0, w1 = w[:dim, :], w[dim:, :]\n",
    "#                 w0 = np.transpose(w0)\n",
    "#                 w1 = np.transpose(w1)\n",
    "                weights.append((k, w0))\n",
    "                weights.append((k, w1))\n",
    "            else:\n",
    "#                 if 'dense' in k:\n",
    "#                     w = np.transpose(w)\n",
    "                weights.append((k, w))\n",
    "    if 'relative_attention_bias' in weights[3][0]:\n",
    "        weights[3], weights[4] = weights[4], weights[3]\n",
    "    weights = [x[1] for x in weights]\n",
    "    if 'encoder' == t:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(encoder_names0 if n == 0 else encoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    else:\n",
    "        weights_dict = OrderedDict()\n",
    "        for k, v in zip(decoder_names0 if n == 0 else decoder_names, weights):\n",
    "            weights_dict[k.format(n)] = v\n",
    "        weights = weights_dict\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../converted.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new_weights = OrderedDict()\n",
    "model_new_weights['shared.weight'] = get_weight('word_embeds.weight')\n",
    "model_new_weights['encoder.embed_tokens.weight'] = get_weight('encoder.word_embeds.weight')\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='encoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['encoder.final_layer_norm.weight'] = get_weight('encoder.final_layernorm.weight')\n",
    "\n",
    "for i in range(24):\n",
    "    for k, v in get_block_weight(i, t='decoder').items():\n",
    "        model_new_weights[k] = v\n",
    "\n",
    "model_new_weights['decoder.final_layer_norm.weight'] = get_weight('decoder.final_layernorm.weight')\n",
    "\n",
    "model_new_weights['decoder.embed_tokens.weight'] = get_weight('decoder.word_embeds.weight')\n",
    "model_new_weights['lm_head.weight'] = get_weight('lm_head.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model.state_dict().keys()) - set(model_new_weights.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(model_new_weights.keys()) - set(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_embeds.weight torch.Size([26240, 4096])\n",
      "lm_head.weight torch.Size([26240, 4096])\n",
      "encoder.word_embeds.weight torch.Size([26240, 4096])\n",
      "encoder.final_layernorm.weight torch.Size([4096])\n",
      "encoder.blocks.0.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 64])\n",
      "encoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.0.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.0.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.1.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.1.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.1.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.2.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.2.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.2.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.3.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.3.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.3.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.4.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.4.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.4.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.5.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.5.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.5.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.6.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.6.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.6.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.7.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.7.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.7.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.8.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.8.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.8.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.9.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.9.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.9.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.10.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.10.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.10.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.11.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.11.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.11.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.12.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.12.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.12.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.13.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.13.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.13.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.14.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.14.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.14.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.15.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.15.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.15.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.16.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.16.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.16.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.17.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.17.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.17.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.18.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.18.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.18.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.19.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.19.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.19.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.20.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.20.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.20.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.21.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.21.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.21.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.22.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.22.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.22.ff.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.23.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "encoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "encoder.blocks.23.self_attn.layer_norm.weight torch.Size([4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.blocks.23.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.word_embeds.weight torch.Size([26240, 4096])\n",
      "decoder.final_layernorm.weight torch.Size([4096])\n",
      "decoder.blocks.0.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.0.self_attn.self_attn.relative_attention_bias.weight torch.Size([32, 64])\n",
      "decoder.blocks.0.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.0.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.0.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.0.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.0.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.1.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.1.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.1.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.1.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.1.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.2.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.2.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.2.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.2.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.2.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.3.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.3.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.3.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.3.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.3.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.4.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.4.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.4.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.4.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.4.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.5.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.5.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.5.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.5.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.5.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.6.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.6.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.6.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.6.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.6.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.7.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.7.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.7.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.7.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.7.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.8.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.8.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.8.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.8.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.8.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.9.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.9.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.9.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.9.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.9.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.10.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.10.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.10.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.10.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.10.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.11.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.11.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.11.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.11.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.11.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.12.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.12.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.12.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.12.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.12.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.13.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.13.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.13.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.13.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.13.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.14.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.14.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.14.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.14.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.14.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.15.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.15.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.15.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.15.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.15.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.16.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.16.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.16.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.16.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.16.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.17.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.17.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.17.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.17.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.17.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.18.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.18.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.18.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.18.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.18.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.19.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.19.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.19.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.19.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.19.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.20.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.20.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.20.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.20.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.20.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.21.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.21.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.21.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.21.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.21.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.22.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.22.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.22.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.22.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.22.ff.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.self_attn.self_attn.project.weight torch.Size([12288, 4096])\n",
      "decoder.blocks.23.self_attn.self_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.self_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_q.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.project_kv.weight torch.Size([8192, 4096])\n",
      "decoder.blocks.23.cross_attn.cross_attn.dense.weight torch.Size([4096, 4096])\n",
      "decoder.blocks.23.cross_attn.layer_norm.weight torch.Size([4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.blocks.23.ff.dense_relu_dense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.blocks.23.ff.layer_norm.weight torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight torch.Size([26240, 4096])\n",
      "encoder.embed_tokens.weight torch.Size([26240, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 64])\n",
      "encoder.block.0.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.0.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.1.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.1.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.2.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.2.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.3.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.3.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.4.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.4.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.5.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.5.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.6.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.6.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.7.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.7.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.8.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.8.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.9.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.9.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.10.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.10.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.11.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.11.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.12.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.12.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.13.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.13.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.14.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.14.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.15.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.15.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.16.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.16.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.17.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.17.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.18.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.18.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.19.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.19.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.20.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.20.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.21.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.21.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.22.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.22.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "encoder.block.23.layer.0.layer_norm.weight torch.Size([4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "encoder.block.23.layer.1.layer_norm.weight torch.Size([4096])\n",
      "encoder.final_layer_norm.weight torch.Size([4096])\n",
      "decoder.embed_tokens.weight torch.Size([26240, 4096])\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight torch.Size([32, 64])\n",
      "decoder.block.0.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.0.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.0.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.1.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.1.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.2.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.2.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.3.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.3.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.4.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.4.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.5.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.5.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.6.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.6.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.7.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.7.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.8.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.8.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.9.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.9.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.10.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.10.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.11.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.11.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.12.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.12.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.12.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.12.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.13.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.13.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.13.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.13.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.14.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.14.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.14.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.14.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.15.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.15.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.15.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.15.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.16.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.16.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.16.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.16.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.17.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.17.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.17.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.17.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.18.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.18.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.18.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.18.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.19.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.19.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.19.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.19.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.20.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.20.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.20.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.20.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.21.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.21.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.21.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.21.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.22.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.22.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.22.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.22.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.23.layer.0.SelfAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.0.SelfAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.0.SelfAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.0.SelfAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.0.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.23.layer.1.EncDecAttention.q.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.1.EncDecAttention.k.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.1.EncDecAttention.v.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.1.EncDecAttention.o.weight torch.Size([4096, 4096])\n",
      "decoder.block.23.layer.1.layer_norm.weight torch.Size([4096])\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_0.weight torch.Size([10240, 4096])\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_1.weight torch.Size([10240, 4096])\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight torch.Size([4096, 10240])\n",
      "decoder.block.23.layer.2.layer_norm.weight torch.Size([4096])\n",
      "decoder.final_layer_norm.weight torch.Size([4096])\n",
      "lm_head.weight torch.Size([26240, 4096])\n"
     ]
    }
   ],
   "source": [
    "for k, v in model.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight (26240, 4096)\n",
      "encoder.embed_tokens.weight (26240, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 64)\n",
      "encoder.block.0.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.0.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.1.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.1.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.1.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.2.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.2.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.2.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.3.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.3.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.3.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.4.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.4.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.4.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.5.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.5.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.5.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.6.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.6.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.6.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.6.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.7.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.7.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.7.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.7.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.8.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.8.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.8.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.8.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.8.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.9.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.9.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.9.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.9.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.9.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.10.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.10.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.10.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.10.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.10.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.11.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.11.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.11.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.11.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.11.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.12.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.12.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.12.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.12.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.12.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.13.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.13.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.13.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.13.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.13.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.14.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.14.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.14.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.14.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.14.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.15.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.15.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.15.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.15.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.15.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.16.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.16.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.16.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.16.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.16.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.17.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.17.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.17.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.17.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.17.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.18.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.18.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.18.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.18.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.18.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.19.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.19.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.19.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.19.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.19.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.20.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.20.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.20.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.20.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.20.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.21.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.21.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.21.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.21.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.21.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.22.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.22.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.22.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.22.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.22.layer.1.layer_norm.weight (4096,)\n",
      "encoder.block.23.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "encoder.block.23.layer.0.layer_norm.weight (4096,)\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "encoder.block.23.layer.1.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "encoder.block.23.layer.1.DenseReluDense.wo.weight (4096, 10240)\n",
      "encoder.block.23.layer.1.layer_norm.weight (4096,)\n",
      "encoder.final_layer_norm.weight (4096,)\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (32, 64)\n",
      "decoder.block.0.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.0.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.0.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.0.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.1.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.1.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.1.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.1.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.2.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.2.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.2.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.2.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.3.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.3.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.3.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.3.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.4.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.4.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.4.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.4.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.5.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.5.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.5.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.5.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.6.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.6.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.6.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.6.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.6.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.6.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.6.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.6.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.6.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.6.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.6.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.6.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.7.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.7.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.7.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.7.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.7.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.7.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.7.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.7.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.7.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.7.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.7.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.7.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.8.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.8.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.8.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.8.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.8.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.8.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.8.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.8.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.8.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.8.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.8.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.8.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.8.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.9.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.9.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.9.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.9.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.9.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.9.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.9.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.9.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.9.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.9.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.9.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.9.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.9.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.10.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.10.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.10.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.10.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.10.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.10.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.10.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.10.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.10.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.10.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.10.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.10.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.10.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.11.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.11.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.11.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.11.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.11.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.11.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.11.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.11.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.11.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.11.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.11.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.11.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.11.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.12.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.12.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.12.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.12.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.12.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.12.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.12.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.12.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.12.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.12.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.12.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.12.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.12.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.13.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.13.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.13.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.13.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.13.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.13.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.13.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.13.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.13.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.13.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.13.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.13.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.13.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.14.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.14.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.14.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.14.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.14.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.14.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.14.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.14.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.14.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.14.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.14.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.14.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.14.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.15.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.15.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.15.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.15.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.15.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.15.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.15.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.15.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.15.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.15.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.15.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.15.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.15.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.16.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.16.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.16.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.16.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.16.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.16.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.16.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.16.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.16.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.16.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.16.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.16.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.16.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.17.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.17.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.17.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.17.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.17.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.17.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.17.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.17.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.17.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.17.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.17.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.17.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.17.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.18.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.18.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.18.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.18.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.18.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.18.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.18.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.18.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.18.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.18.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.18.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.18.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.18.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.19.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.19.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.19.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.19.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.19.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.19.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.19.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.19.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.19.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.19.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.19.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.19.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.19.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.20.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.20.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.20.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.20.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.20.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.20.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.20.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.20.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.20.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.20.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.20.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.20.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.20.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.21.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.21.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.21.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.21.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.21.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.21.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.21.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.21.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.21.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.21.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.21.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.21.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.21.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.22.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.22.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.22.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.22.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.22.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.22.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.22.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.22.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.22.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.22.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.22.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.22.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.22.layer.2.layer_norm.weight (4096,)\n",
      "decoder.block.23.layer.0.SelfAttention.q.weight (4096, 4096)\n",
      "decoder.block.23.layer.0.SelfAttention.k.weight (4096, 4096)\n",
      "decoder.block.23.layer.0.SelfAttention.v.weight (4096, 4096)\n",
      "decoder.block.23.layer.0.SelfAttention.o.weight (4096, 4096)\n",
      "decoder.block.23.layer.0.layer_norm.weight (4096,)\n",
      "decoder.block.23.layer.1.EncDecAttention.q.weight (4096, 4096)\n",
      "decoder.block.23.layer.1.EncDecAttention.k.weight (4096, 4096)\n",
      "decoder.block.23.layer.1.EncDecAttention.v.weight (4096, 4096)\n",
      "decoder.block.23.layer.1.EncDecAttention.o.weight (4096, 4096)\n",
      "decoder.block.23.layer.1.layer_norm.weight (4096,)\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_0.weight (10240, 4096)\n",
      "decoder.block.23.layer.2.DenseReluDense.wi_1.weight (10240, 4096)\n",
      "decoder.block.23.layer.2.DenseReluDense.wo.weight (4096, 10240)\n",
      "decoder.block.23.layer.2.layer_norm.weight (4096,)\n",
      "decoder.final_layer_norm.weight (4096,)\n",
      "decoder.embed_tokens.weight (26240, 4096)\n",
      "lm_head.weight (26240, 4096)\n"
     ]
    }
   ],
   "source": [
    "for k, v in model_new_weights.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(model_new_weights) == len(model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict({k: torch.from_numpy(v) for k, v in model_new_weights.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''当地时间9月6日是美国劳工节，但就在这一天，上千万美国劳动者却陷入新的困境。因为美国政府为疫情期间失业者提供的主要救助同日到期，而且白宫表示没有进一步延长救助的计划。\n",
    "在德尔塔变异株已把美国推入新一轮疫情的背景下，失业救济的突然“断供”意味着有上千万美国人将全部或部分失去他们的生活来源。'''\n",
    "input_ids = torch.LongTensor([tokenizer.encode(input_text) + [tokenizer.get_sentinel_id(0)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    decoder_start_token_id=1,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    top_k=20,\n",
    "    bad_words_ids=[[x] for x in range(26050, tokenizer.vocab_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_text = tokenizer.decode(out.detach().numpy()[0].tolist()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加油的希望被打破了。\n",
      "为什么会这样？ \n",
      " “美国政府将在今明两天“断供”？\n",
      " \n",
      "美国政府为疫情期间失业救被断  \n",
      " \n",
      "美国政府\n"
     ]
    }
   ],
   "source": [
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "美国劳动\n",
      "美国劳动者\n",
      "美国劳动者的\n",
      "美国劳动者的新\n",
      "美国劳动者的新困\n",
      "美国劳动者的新困境\n",
      "美国劳动者的新困境。\n",
      "美国劳动者的新困境。\n",
      "\n",
      "美国劳动者的新困境。\n",
      "美国\n",
      "美国劳动者的新困境。\n",
      "美国政府\n",
      "美国劳动者的新困境。\n",
      "美国政府为\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业者\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业者提供\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业者提供的\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业者提供的主要\n",
      "美国劳动者的新困境。\n",
      "美国政府为疫情期间失业者提供的主要救助\n"
     ]
    }
   ],
   "source": [
    "input_text = '''当地时间9月6日是美国劳工节，但就在这一天，上千万美国劳动者却陷入新的困境。因为美国政府为疫情期间失业者提供的主要救助同日到期，而且白宫表示没有进一步延长救助的计划。\n",
    "在德尔塔变异株已把美国推入新一轮疫情的背景下，失业救济的突然“断供”意味着有上千万美国人将全部或部分失去他们的生活来源。'''\n",
    "output_text = '''美国'''\n",
    "input_ids = torch.LongTensor([tokenizer.encode(input_text)])\n",
    "\n",
    "for i in range(20):\n",
    "    decoder_input_ids = torch.LongTensor([[1, tokenizer.get_sentinel_id(0)] + tokenizer.encode(output_text)])\n",
    "    out = model(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "    t = out['logits'][:, -1, :26050].detach().numpy()\n",
    "    next_token = tokenizer.decode([np.argmax(t, -1)[0]])\n",
    "    # print(next_token)\n",
    "    output_text += next_token\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
